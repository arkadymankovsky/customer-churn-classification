{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df5ac6c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b764e7b8167f0a2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T07:34:11.864360Z",
     "start_time": "2025-01-28T07:34:10.299557Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, classification_report, confusion_matrix, balanced_accuracy_score\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.feature_utils import generate_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496ed74",
   "metadata": {},
   "source": [
    "# Auxiliray Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43b60a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_summary_table(data_frame: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Creates a Pandas DataFrame summarizing basic EDA information for each column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with EDA summary (% missing, % unique, n_unique, column type).\n",
    "    \"\"\"\n",
    "    eda_list = []\n",
    "    for col in data_frame.columns:\n",
    "        missing_percent = (data_frame[col].isnull().sum() / len(data_frame)) * 100\n",
    "        unique_percent = (data_frame[col].nunique() / len(data_frame)) * 100\n",
    "        n_unique = data_frame[col].nunique()\n",
    "        dtype = data_frame[col].dtype\n",
    "        eda_list.append([missing_percent, unique_percent, n_unique, dtype])\n",
    "\n",
    "    eda_table = pd.DataFrame(\n",
    "        eda_list, \n",
    "        columns=['% Missing Values', '% Unique Values', 'N Unique Values', 'Column Type'], \n",
    "        index=data_frame.columns\n",
    "    )\n",
    "    # Round percentage columns to 2 decimals\n",
    "    eda_table['% Missing Values'] = eda_table['% Missing Values'].round(2)\n",
    "    eda_table['% Unique Values'] = eda_table['% Unique Values'].round(2)\n",
    "    return eda_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fad0f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplictated_rows(data_frame: pd.DataFrame):\n",
    "    total_duplicates = data_frame.duplicated().sum()\n",
    "    total_duplicate_percent = (total_duplicates / len(data_frame)) * 100\n",
    "    print(f\"\\nTotal duplicate rows: {total_duplicates} ({total_duplicate_percent:.2f}% of dataset)\")\n",
    "\n",
    "    # Display duplicate combinations of customer_id and date\n",
    "    customer_date_duplicates = data_frame.duplicated(subset=['customer_id', 'date']).sum()\n",
    "    customer_date_duplicate_percent = (customer_date_duplicates / len(data_frame)) * 100\n",
    "    print(f\"Duplicate user-app combinations: {customer_date_duplicates} ({customer_date_duplicate_percent:.2f}% of dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5edad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_df(data_frame: pd.DataFrame):\n",
    "    print(f'Number of rows: {data_frame.shape[0]}, Number of columns: {data_frame.shape[1]}')\n",
    "    print(f'Number of duplicated rows: data_frame.duplicated().sum()')\n",
    "    print(\"First 2 rows:\")\n",
    "    display(data_frame.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "825f9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_numeric_values(series):\n",
    "    non_numeric = series[pd.to_numeric(series, errors='coerce').isna() & series.notna()]\n",
    "    return non_numeric.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe099a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(values: pd.Series, \n",
    "                     title: str,\n",
    "                     log_scale: bool = False, \n",
    "                     figsize: tuple = (12, 8),\n",
    "                     bins: int = 30) -> None:\n",
    "    \"\"\"\n",
    "    Plot distribution and boxplot for a given series of values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    values : pd.Series\n",
    "        Values to plot\n",
    "    title : str\n",
    "        Title for the plot\n",
    "    log_scale : bool, default=False\n",
    "        Whether to use log scale for the plots\n",
    "    figsize : tuple, default=(12, 8)\n",
    "        Figure size in inches (width, height)\n",
    "    bins : int, default=30\n",
    "        Number of bins for the histogram\n",
    "    \"\"\"\n",
    "    # Create figure with stacked subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, height_ratios=[2, 1])\n",
    "    \n",
    "    # Plot histogram\n",
    "    sns.histplot(values, bins=bins, ax=ax1)\n",
    "    ax1.set_title(f'Distribution of {title}')\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    if log_scale:\n",
    "        ax1.set_yscale('log')\n",
    "    \n",
    "    # Plot boxplot\n",
    "    sns.boxplot(x=values, ax=ax2)\n",
    "    ax2.set_xlabel(title)\n",
    "    \n",
    "    if log_scale:\n",
    "        ax2.set_xscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eee37ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permission_distribution(data_frame: pd.DataFrame):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # First subplot - Permissions by Seniority\n",
    "    perm_by_seniority = data_frame.groupby('seniority_inverted', observed=True)['permission'].value_counts(normalize=True).unstack() * 100\n",
    "    perm_by_seniority.plot(kind='bar', width=0.8, ax=ax1)\n",
    "\n",
    "    ax1.set_title('Permission Distribution by Seniority Level')\n",
    "    ax1.set_xlabel('Seniority Level')\n",
    "    ax1.set_ylabel('Percentage')\n",
    "    ax1.legend(title='Permission', labels=['Denied (0)', 'Granted (1)'])\n",
    "\n",
    "    # Add percentage labels on the bars for first subplot\n",
    "    for container in ax1.containers:\n",
    "        ax1.bar_label(container, \n",
    "                    label_type='center',\n",
    "                    fmt='%.1f%%',\n",
    "                    color='white',\n",
    "                    fontweight='bold')\n",
    "\n",
    "    # Second subplot - Permissions by isMachine\n",
    "    perm_by_machine = data_frame.groupby('isMachine', observed=True)['permission'].value_counts(normalize=True).unstack() * 100\n",
    "    perm_by_machine.plot(kind='bar', width=0.8, ax=ax2)\n",
    "\n",
    "    ax2.set_title('Permission Distribution by Machine Status')\n",
    "    ax2.set_xlabel('Is Machine')\n",
    "    ax2.set_ylabel('Percentage')\n",
    "    ax2.legend(title='Permission', labels=['Denied (0)', 'Granted (1)'])\n",
    "\n",
    "    # Add percentage labels on the bars for second subplot\n",
    "    for container in ax2.containers:\n",
    "        ax2.bar_label(container, \n",
    "                    label_type='center',\n",
    "                    fmt='%.1f%%',\n",
    "                    color='white',\n",
    "                    fontweight='bold')\n",
    "\n",
    "    # Adjust x-axis labels for isMachine\n",
    "    ax2.set_xticklabels(['Human (0)', 'Machine (1)'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "463d3954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_is_machine_seniority_distribution(data_frame: pd.DataFrame):\n",
    "    machine_by_seniority = data_frame.groupby('seniority_inverted', observed=True)['isMachine'].value_counts(normalize=True).unstack() * 100\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = machine_by_seniority.plot(kind='bar', width=0.8)\n",
    "\n",
    "    plt.title('Distribution of Machine vs. Human Users by Seniority Level')\n",
    "    plt.xlabel('Seniority Level')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.legend(title='Is Machine', labels=['Human (0)', 'Machine (1)'])\n",
    "\n",
    "    # Add percentage labels on the bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, \n",
    "                    label_type='center',\n",
    "                    fmt='%.1f%%',\n",
    "                    color='white',\n",
    "                    fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04568250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permission_by_dep_cat_distributions(data_frame: pd.DataFrame):\n",
    "    # Create figure with a 2x2 grid\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(2, 2)\n",
    "    \n",
    "    # Create four axes\n",
    "    ax1 = fig.add_subplot(gs[0, 0])  # top left\n",
    "    ax2 = fig.add_subplot(gs[0, 1])  # top right\n",
    "    ax3 = fig.add_subplot(gs[1, 0])  # bottom left\n",
    "    ax4 = fig.add_subplot(gs[1, 1])  # bottom right\n",
    "    \n",
    "    # Plot 1: Categories (top left)\n",
    "    perm_by_category = data_frame.groupby('category', observed=False)['permission'].value_counts(normalize=True).unstack() * 100\n",
    "    perm_by_category = perm_by_category.sort_values(by=1, ascending=True)\n",
    "    perm_by_category.plot(kind='barh', width=0.8, ax=ax1)\n",
    "    ax1.set_title('Permission Distribution by Category')\n",
    "    ax1.set_xlabel('Percentage')\n",
    "    ax1.set_ylabel('Category')\n",
    "    ax1.legend(title='Permission', labels=['Denied (0)', 'Granted (1)'])\n",
    "    \n",
    "    # Plot 2: Departments (top right)\n",
    "    perm_by_dept = data_frame.groupby('department', observed=False)['permission'].value_counts(normalize=True).unstack() * 100\n",
    "    perm_by_dept = perm_by_dept.sort_values(by=1, ascending=True)\n",
    "    perm_by_dept.plot(kind='barh', width=0.8, ax=ax2)\n",
    "    ax2.set_title('Permission Distribution by Department')\n",
    "    ax2.set_xlabel('Percentage')\n",
    "    ax2.set_ylabel('Department')\n",
    "    ax2.legend(title='Permission', labels=['Denied (0)', 'Granted (1)'])\n",
    "    \n",
    "    # Plot 3: Office Locations (bottom left)\n",
    "    perm_by_location = data_frame.groupby('officeLocation', observed=False)['permission'].value_counts(normalize=True).unstack() * 100\n",
    "    perm_by_location = perm_by_location.sort_values(by=1, ascending=True)\n",
    "    perm_by_location.plot(kind='barh', width=0.8, ax=ax3)\n",
    "    ax3.set_title('Permission Distribution by Office Location')\n",
    "    ax3.set_xlabel('Percentage')\n",
    "    ax3.set_ylabel('Office Location')\n",
    "    ax3.legend(title='Permission', labels=['Denied (0)', 'Granted (1)'])\n",
    "    \n",
    "    # Plot 4: Permission Rates by App (bottom right)\n",
    "    app_stats = data_frame.groupby('appId', observed=False)['permission'].agg(['mean', 'count'])\n",
    "    app_stats = app_stats.sort_values('mean', ascending=True)\n",
    "    \n",
    "    # Plot permission rates\n",
    "    app_stats['mean'].plot(kind='hist', bins=30, ax=ax4)\n",
    "    ax4.set_title('Distribution of Permission Rates Across Apps')\n",
    "    ax4.set_xlabel('Permission Rate')\n",
    "    ax4.set_ylabel('Count of Apps')\n",
    "    \n",
    "    # Add mean and median lines\n",
    "    mean_rate = app_stats['mean'].mean()\n",
    "    median_rate = app_stats['mean'].median()\n",
    "    \n",
    "    # Add mean line (red)\n",
    "    ax4.axvline(x=mean_rate, color='r', linestyle='--', alpha=0.7, label=f'Mean: {mean_rate:.1%}')\n",
    "    \n",
    "    # Add median line (blue)\n",
    "    ax4.axvline(x=median_rate, color='b', linestyle='--', alpha=0.7, label=f'Median: {median_rate:.1%}')\n",
    "    \n",
    "    # Add legend for mean and median lines\n",
    "    ax4.legend()\n",
    "    \n",
    "    # Add percentage labels on bars for first three plots\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, \n",
    "                        label_type='center',\n",
    "                        fmt='%.1f%%',\n",
    "                        color='white',\n",
    "                        fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2804ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permission_rate_and_isMachine_dist(data_frame: pd.DataFrame):\n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot 1: Overall Permission Distribution\n",
    "    permission_dist = data_frame['permission'].value_counts(normalize=True) * 100\n",
    "    permission_dist.plot(kind='bar', ax=ax1)\n",
    "    ax1.set_title('Overall Permission Distribution')\n",
    "    ax1.set_xlabel('Permission')\n",
    "    ax1.set_ylabel('Percentage')\n",
    "    ax1.set_xticklabels(['Denied (0)', 'Granted (1)'])\n",
    "\n",
    "    # Add percentage labels on bars\n",
    "    for i, v in enumerate(permission_dist):\n",
    "        ax1.text(i, v/2, f'{v:.1f}%', ha='center', color='white', fontweight='bold')\n",
    "\n",
    "    # Plot 2: isMachine Distribution\n",
    "    machine_dist = data_frame['isMachine'].value_counts(normalize=True) * 100\n",
    "    machine_dist.plot(kind='bar', ax=ax2)\n",
    "    ax2.set_title('Machine vs Human Distribution')\n",
    "    ax2.set_xlabel('Is Machine')\n",
    "    ax2.set_ylabel('Percentage')\n",
    "    ax2.set_xticklabels(['Human (0)', 'Machine (1)'])\n",
    "\n",
    "    # Add percentage labels with adjusted position for small values\n",
    "    for i, v in enumerate(machine_dist):\n",
    "        if v < 10:  # For small values, place text above the bar\n",
    "            ax2.text(i, v + 1, f'{v:.1f}%', ha='center', color='black', fontweight='bold')\n",
    "        else:  # For larger values, place text in the middle of the bar\n",
    "            ax2.text(i, v/2, f'{v:.1f}%', ha='center', color='white', fontweight='bold')\n",
    "\n",
    "    # Adjust y-axis to make room for labels above small bars\n",
    "    ax2.set_ylim(0, max(machine_dist) * 1.1)  # Add 10% padding at the top\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "821e6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seniority_distribution(data_frame: pd.DataFrame):\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Calculate and sort seniority distribution\n",
    "    seniority_dist = data_frame['seniority_inverted'].value_counts(normalize=True) * 100\n",
    "    seniority_dist = seniority_dist.sort_index()  # Sort by seniority level\n",
    "\n",
    "    # Create bar plot\n",
    "    ax = seniority_dist.plot(kind='bar')\n",
    "    plt.title('Seniority Level Distribution')\n",
    "    plt.xlabel('Seniority Level')\n",
    "    plt.ylabel('Percentage')\n",
    "\n",
    "    # Add percentage labels with adjusted position for small values\n",
    "    for i, v in enumerate(seniority_dist):\n",
    "        if v < 10:  # For small values, place text above the bar\n",
    "            plt.text(i, v + 0.5, f'{v:.1f}%', ha='center', color='black', fontweight='bold')\n",
    "        else:  # For larger values, place text in the middle of the bar\n",
    "            plt.text(i, v/2, f'{v:.1f}%', ha='center', color='white', fontweight='bold')\n",
    "\n",
    "    # Adjust y-axis to make room for labels above small bars\n",
    "    plt.ylim(0, max(seniority_dist) * 1.1)  # Add 10% padding at the top\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "528ca65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_department(data_frame: pd.DataFrame):\n",
    "    dept_permission_rates = data_frame.groupby('department', observed=True)['permission'].mean().reset_index()\n",
    "    dept_permission_rates.columns = ['department', 'grant_rate']\n",
    "\n",
    "    n_dept_bins = 5\n",
    "    dept_permission_rates['dept_bin'] = pd.qcut(\n",
    "        dept_permission_rates['grant_rate'], \n",
    "        n_dept_bins, \n",
    "        labels=[f'dept_bin_{i+1}' for i in range(n_dept_bins)]\n",
    "    )\n",
    "\n",
    "    dept_to_bin = dict(zip(dept_permission_rates['department'], dept_permission_rates['dept_bin']))\n",
    "\n",
    "    data_frame['department_binned'] = data_frame['department'].map(dept_to_bin)\n",
    "\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "862f103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_office_location(data_frame: pd.DataFrame):\n",
    "    loc_permission_rates = data_frame.groupby('officeLocation', observed=True)['permission'].mean().reset_index()\n",
    "    loc_permission_rates.columns = ['officeLocation', 'grant_rate']\n",
    "\n",
    "    n_loc_bins = 5\n",
    "    loc_permission_rates['loc_bin'] = pd.qcut(\n",
    "        loc_permission_rates['grant_rate'], \n",
    "        n_loc_bins, \n",
    "        labels=[f'loc_bin_{i+1}' for i in range(n_loc_bins)]\n",
    "    )\n",
    "\n",
    "    loc_to_bin = dict(zip(loc_permission_rates['officeLocation'], loc_permission_rates['loc_bin']))\n",
    "\n",
    "    # Step 8: Apply location binning to original dataframe\n",
    "    data_frame['officeLocation_binned'] = data_frame['officeLocation'].map(loc_to_bin)\n",
    "\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f02622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df_train, df_test, categorical_features, numeric_features, model_path=None):\n",
    "    \"\"\"Train CatBoost model with CV and evaluate on test set.\"\"\"\n",
    "    print(\"Feature Information:\")\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    print(f\"Numeric features: {numeric_features}\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_counts = df_train['permission'].value_counts()\n",
    "    n_samples = len(df_train)\n",
    "    n_classes = len(class_counts)\n",
    "    class_weights = {i: n_samples / (n_classes * count) for i, count in class_counts.items()}\n",
    "    \n",
    "    # Good default parameters for CatBoost\n",
    "    params = {\n",
    "        'iterations': 500,\n",
    "        'depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'max_bin': 200,\n",
    "        'random_strength': 1,\n",
    "        'bagging_temperature': 1,\n",
    "        'class_weights': class_weights,\n",
    "        'thread_count': -1,  # Use all CPU cores\n",
    "        'task_type': 'GPU',  # Enable GPU training\n",
    "        'devices': '0',      # Use first GPU device\n",
    "        'verbose': False,\n",
    "        'gpu_ram_part': 0.95,     # Use 95% of GPU memory\n",
    "        'pinned_memory_size': '1gb',\n",
    "        'class_names': [0, 1],\n",
    "    }\n",
    "    \n",
    "    # Prepare features\n",
    "    X = df_train[categorical_features + numeric_features]\n",
    "    y = df_train['permission']\n",
    "    \n",
    "    # Create stratification column\n",
    "    df_train['strat'] = df_train['permission'].astype(str) + '_' + df_train['isMachine'].astype(str)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    print(\"\\nPerforming 5-fold CV...\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, df_train['strat']), 1):\n",
    "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Create pools\n",
    "        train_pool = Pool(\n",
    "            X_fold_train,\n",
    "            y_fold_train,\n",
    "            cat_features=categorical_features\n",
    "        )\n",
    "        val_pool = Pool(\n",
    "            X_fold_val,\n",
    "            y_fold_val,\n",
    "            cat_features=categorical_features\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model = CatBoostClassifier(**params, random_seed=42)\n",
    "        model.fit(train_pool, use_best_model=True)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(val_pool)\n",
    "        fold_score = f1_score(y_fold_val, y_pred)\n",
    "        cv_scores.append(fold_score)\n",
    "        print(f\"Fold {fold} F1 Score: {fold_score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMean CV F1 Score: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "    \n",
    "    # Train final model on full training data\n",
    "    print(\"\\nTraining final model on full training data...\")\n",
    "    final_model = CatBoostClassifier(**params, random_seed=42)\n",
    "    \n",
    "    train_pool = Pool(\n",
    "        X,\n",
    "        y,\n",
    "        cat_features=categorical_features\n",
    "    )\n",
    "    \n",
    "    final_model.fit(train_pool)\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': final_model.feature_names_,\n",
    "        'Importance': final_model.get_feature_importance()\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    print(\"\\nTop 10 Feature Importance:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    X_test = df_test[categorical_features + numeric_features]\n",
    "    y_test = df_test['permission']\n",
    "    \n",
    "    test_pool = Pool(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cat_features=categorical_features\n",
    "    )\n",
    "    \n",
    "    y_pred = final_model.predict(test_pool)\n",
    "    y_pred_proba = final_model.predict_proba(test_pool)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # if roc_auc < 0.5:\n",
    "    #     y_pred_proba = 1 - y_pred_proba\n",
    "    #     fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    #     roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Store all results\n",
    "    test_results = {\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'classification_report': classification_report(y_test, y_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'roc_curve': {\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr\n",
    "        },\n",
    "        'auc_score': roc_auc,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'accuracy': balanced_accuracy_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Print results nicely\n",
    "    print_results(test_results)\n",
    "    \n",
    "    # Save model if path is provided\n",
    "    if model_path:\n",
    "        final_model.save_model(model_path)\n",
    "        print(f\"\\nModel saved to: {model_path}\")\n",
    "    \n",
    "    return final_model, test_results\n",
    "\n",
    "def print_results(results):\n",
    "    \"\"\"Print classification results in a formatted way.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL PERFORMANCE METRICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # F1 Score and AUC Score\n",
    "    print(f\"\\nF1 Score: {results['f1_score']:.4f}\")\n",
    "    print(f\"AUC Score: {results['auc_score']:.4f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nDetailed Performance Metrics:\")\n",
    "    print(\"-\"*50)\n",
    "    print(results['classification_report'])\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = results['confusion_matrix']\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"                Predicted NO  Predicted YES\")\n",
    "    print(f\"Actual NO     {cm[0,0]:>11,d} {cm[0,1]:>13,d}\")\n",
    "    print(f\"Actual YES    {cm[1,0]:>11,d} {cm[1,1]:>13,d}\")\n",
    "    \n",
    "    # Additional metrics\n",
    "    total = cm.sum()\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(\"\\nAdditional Metrics:\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Total Samples:    {total:,d}\")\n",
    "    print(f\"Correct:          {(tn + tp):,d}\")\n",
    "    print(f\"Incorrect:        {(fp + fn):,d}\")\n",
    "    print(f\"Accuracy:         {(tn + tp) / total:.4f}\")\n",
    "    print(f\"Misclass. Rate:   {(fp + fn) / total:.4f}\")\n",
    "    print(f\"Precision:        {tp / (tp + fp):.4f}\")\n",
    "    print(f\"Recall:          {tp / (tp + fn):.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(results['roc_curve']['fpr'], results['roc_curve']['tpr'], \n",
    "             color='darkorange', lw=2,\n",
    "             label=f'ROC curve (AUC = {results[\"auc_score\"]:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c6ffeb",
   "metadata": {},
   "source": [
    "# Section 1 - Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118315c0",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "34e97ace4627a714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T07:34:27.343174Z",
     "start_time": "2025-01-28T07:34:27.253402Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/raw/churn_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be18d31",
   "metadata": {},
   "source": [
    "### data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1353c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1200, Number of columns: 6\n",
      "Number of duplicated rows: data_frame.duplicated().sum()\n",
      "First 2 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>transaction_amount</th>\n",
       "      <th>plan_type</th>\n",
       "      <th>churn</th>\n",
       "      <th>issuing_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUST_1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>193.524658</td>\n",
       "      <td>Basic</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUST_1</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>303.342657</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUST_1</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>38.460970</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUST_1</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>356.955563</td>\n",
       "      <td>Premium</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUST_1</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>417.896894</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CUST_1</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Premium</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CUST_1</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>221.653059</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CUST_1</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>78.351992</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CUST_1</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>233.474292</td>\n",
       "      <td>Premium</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CUST_1</td>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>261.974875</td>\n",
       "      <td>Standard</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id        date  transaction_amount plan_type  churn issuing_date\n",
       "0      CUST_1  2023-01-01          193.524658     Basic      0   2021-03-01\n",
       "1      CUST_1  2023-02-01          303.342657  Standard      0   2021-03-01\n",
       "2      CUST_1  2023-03-01           38.460970  Standard      0   2021-03-01\n",
       "3      CUST_1  2023-04-01          356.955563   Premium      0   2021-03-01\n",
       "4      CUST_1  2023-05-01          417.896894  Standard      0   2021-03-01\n",
       "5      CUST_1  2023-06-01                 NaN   Premium      0   2021-03-01\n",
       "6      CUST_1  2023-07-01          221.653059  Standard      0   2021-03-01\n",
       "7      CUST_1  2023-08-01           78.351992  Standard      0   2021-03-01\n",
       "8      CUST_1  2023-09-01          233.474292   Premium      0   2021-03-01\n",
       "9      CUST_1  2023-10-01          261.974875  Standard      1   2021-03-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6453d65",
   "metadata": {},
   "source": [
    "## Merged data frame inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d51c5ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    % Missing Values  % Unique Values  N Unique Values  \\\n",
      "customer_id                     0.00             8.33              100   \n",
      "date                            0.00             1.00               12   \n",
      "transaction_amount              0.58            99.42             1193   \n",
      "plan_type                       0.08             0.25                3   \n",
      "churn                           0.00             0.17                2   \n",
      "issuing_date                    0.00             4.42               53   \n",
      "\n",
      "                   Column Type  \n",
      "customer_id             object  \n",
      "date                    object  \n",
      "transaction_amount     float64  \n",
      "plan_type               object  \n",
      "churn                    int64  \n",
      "issuing_date            object  \n",
      "\n",
      "Total duplicate rows: 0 (0.00% of dataset)\n",
      "Duplicate user-app combinations: 0 (0.00% of dataset)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(create_column_summary_table(df))\n",
    "print(count_duplictated_rows(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea588629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_consecutive_churn(df):\n",
    "    \"\"\"\n",
    "    Efficiently identifies customers with non-consecutive churn patterns.\n",
    "    Returns a list of customer IDs where churn goes from 1 back to 0.\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe with only necessary columns\n",
    "    temp_df = df[['customer_id', 'date', 'churn']].copy()\n",
    "    \n",
    "    # Convert date to datetime and sort\n",
    "    temp_df['date'] = pd.to_datetime(temp_df['date'])\n",
    "    temp_df = temp_df.sort_values(['customer_id', 'date'])\n",
    "    \n",
    "    # Create a shifted column to compare consecutive values\n",
    "    temp_df['prev_churn'] = temp_df.groupby('customer_id')['churn'].shift(1)\n",
    "    \n",
    "    # Find rows where churn went from 1 to 0\n",
    "    invalid_transitions = temp_df[(temp_df['prev_churn'] == 1) & (temp_df['churn'] == 0)]\n",
    "    \n",
    "    # Get unique customer IDs with invalid transitions\n",
    "    non_consecutive_customers = invalid_transitions['customer_id'].unique().tolist()\n",
    "    \n",
    "    print(f\"Found {len(non_consecutive_customers)} customers with non-consecutive churn patterns\")\n",
    "    \n",
    "    return non_consecutive_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6692bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 customers with non-consecutive churn patterns\n"
     ]
    }
   ],
   "source": [
    "non_consecutive_customers = find_non_consecutive_churn(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7f79d69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>transaction_amount</th>\n",
       "      <th>plan_type</th>\n",
       "      <th>churn</th>\n",
       "      <th>issuing_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>145.323083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    customer_id        date  transaction_amount plan_type  churn issuing_date\n",
       "948     CUST_80  2023-01-01          145.323083       NaN      0   2021-12-01"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.plan_type.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29884a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('customer_id').size().to_frame()[0].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "295ee774",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = generate_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26277a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "288bcd11",
   "metadata": {},
   "source": [
    "last_plan_type: The plan type in the most recent month (December 2023)\n",
    "is_churned_last_month: Whether the customer was in churned state in December 2023\n",
    "tenure_months: Number of months the customer has been with the service as of December 2023 ---\n",
    "days_since_plan_change: Number of days since the customer last changed their plan\n",
    "mom_transaction_change: Month-over-month percentage change in transaction amount (Nov to Dec) ---- less\n",
    "recent_volatility: Standard deviation of transaction amounts in the last 3 months ---- can catch neg' or pos'?\n",
    "avg_transaction_amount: Average transaction amount across all months ---- how predictive?\n",
    "transaction_trend: Slope coefficient of transaction amounts over time (increasing/decreasing) ---- can be important\n",
    "total_plan_changes: Total number of times the customer changed plans during the year\n",
    "pct_basic_plan: Percentage of months the customer was on the Basic plan\n",
    "pct_standard_plan: Percentage of months the customer was on the Standard plan\n",
    "pct_premium_plan: Percentage of months the customer was on the Premium plan\n",
    "last_plan_change_type: Direction of the most recent plan change (1=upgrade, -1=downgrade, 0=no change)\n",
    "missing_transaction_months: Number of months with missing transaction data\n",
    "transaction_cv: Coefficient of variation of transaction amounts (std/mean)\n",
    "max_transaction_amount: Maximum transaction amount recorded ---- less\n",
    "min_transaction_amount: Minimum transaction amount recorded ---- less\n",
    "q1_avg_transaction: Average transaction amount in Q1 (Jan-Mar)\n",
    "q2_avg_transaction: Average transaction amount in Q2 (Apr-Jun)\n",
    "q3_avg_transaction: Average transaction amount in Q3 (Jul-Sep)\n",
    "q4_avg_transaction: Average transaction amount in Q4 (Oct-Dec)\n",
    "recent_to_historical_ratio: Ratio of recent 6 months' spending to earlier 6 months"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1e390",
   "metadata": {},
   "source": [
    "tenure_months\n",
    "transaction_trend\n",
    "pct_basic_plan\n",
    "pct_standard_plan\n",
    "pct_premium_plan\n",
    "last_plan_change_type\n",
    "missing_transaction_months\n",
    "transaction_cv\n",
    "q3_avg_transaction\n",
    "q4_avg_transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2df91",
   "metadata": {},
   "source": [
    "tenure_months\n",
    "transaction_trend\n",
    "mom_transaction_change\n",
    "recent_volatility\n",
    "last_plan_change_type\n",
    "total_plan_changes\n",
    "days_since_plan_change\n",
    "transaction_cv\n",
    "recent_to_historical_ratio\n",
    "pct_premium_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51fc3433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id                      CUST_80\n",
       "last_plan_type                  Standard\n",
       "is_churned_last_month                  0\n",
       "tenure_months                  25.333333\n",
       "days_since_plan_change                60\n",
       "mom_transaction_change         -0.592473\n",
       "recent_volatility             124.746267\n",
       "avg_transaction_amount        291.655959\n",
       "transaction_trend               3.650623\n",
       "total_plan_changes                     9\n",
       "pct_basic_plan                  0.333333\n",
       "pct_standard_plan               0.333333\n",
       "pct_premium_plan                    0.25\n",
       "last_plan_change_type                  0\n",
       "missing_transaction_months             0\n",
       "transaction_cv                  0.400841\n",
       "max_transaction_amount        482.398424\n",
       "min_transaction_amount        145.323083\n",
       "q1_avg_transaction            313.213977\n",
       "q2_avg_transaction            190.875212\n",
       "q3_avg_transaction             363.08576\n",
       "q4_avg_transaction            299.448888\n",
       "recent_to_historical_ratio       1.31432\n",
       "Name: 79, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.query('customer_id==\"CUST_80\"').iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "11a76c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>transaction_amount</th>\n",
       "      <th>plan_type</th>\n",
       "      <th>churn</th>\n",
       "      <th>issuing_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>145.323083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>421.275861</td>\n",
       "      <td>Basic</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>373.042988</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>186.514620</td>\n",
       "      <td>Basic</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>163.540480</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>222.570536</td>\n",
       "      <td>Premium</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>482.398424</td>\n",
       "      <td>Basic</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>251.968880</td>\n",
       "      <td>Basic</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>354.889975</td>\n",
       "      <td>Premium</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>359.392565</td>\n",
       "      <td>Premium</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>382.908517</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>CUST_80</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>156.045582</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    customer_id        date  transaction_amount plan_type  churn issuing_date\n",
       "948     CUST_80  2023-01-01          145.323083       NaN      0   2021-12-01\n",
       "949     CUST_80  2023-02-01          421.275861     Basic      0   2021-12-01\n",
       "950     CUST_80  2023-03-01          373.042988  Standard      0   2021-12-01\n",
       "951     CUST_80  2023-04-01          186.514620     Basic      0   2021-12-01\n",
       "952     CUST_80  2023-05-01          163.540480  Standard      0   2021-12-01\n",
       "953     CUST_80  2023-06-01          222.570536   Premium      0   2021-12-01\n",
       "954     CUST_80  2023-07-01          482.398424     Basic      0   2021-12-01\n",
       "955     CUST_80  2023-08-01          251.968880     Basic      0   2021-12-01\n",
       "956     CUST_80  2023-09-01          354.889975   Premium      0   2021-12-01\n",
       "957     CUST_80  2023-10-01          359.392565   Premium      0   2021-12-01\n",
       "958     CUST_80  2023-11-01          382.908517  Standard      0   2021-12-01\n",
       "959     CUST_80  2023-12-01          156.045582  Standard      0   2021-12-01"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('customer_id==\"CUST_80\"').sort_values('date')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
